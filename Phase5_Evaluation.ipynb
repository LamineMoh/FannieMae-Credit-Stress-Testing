{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 5: Evaluation\n",
                "\n",
                "## Fannie Mae 2008Q1 Stress Testing - Credit Default Risk Modeling\n",
                "\n",
                "---\n",
                "\n",
                "### CRISP-DM Phase 5: Evaluate and Compare Model Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pickle\n",
                "from sklearn.metrics import (\n",
                "    classification_report, confusion_matrix, roc_curve, \n",
                "    precision_recall_curve, roc_auc_score\n",
                ")\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "%matplotlib inline\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.1 Load Results from Phase 4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load results\n",
                "with open('phase4_results.pkl', 'rb') as f:\n",
                "    data = pickle.load(f)\n",
                "\n",
                "results = data['results']\n",
                "comparison_df = data['comparison_df']\n",
                "best_model_name = data['best_model_name']\n",
                "y_test = data['y_test']\n",
                "features = data['features']\n",
                "\n",
                "print(f\"Loaded results for {len(results)} models\")\n",
                "print(f\"Best model: {best_model_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.2 Model Comparison Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display comparison with highlighting\n",
                "print(\"Model Performance Comparison:\")\n",
                "print(\"=\"*60)\n",
                "display(comparison_df.round(4).style.highlight_max(axis=0, color='lightgreen'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.3 ROC Curves Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot ROC curves\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "\n",
                "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
                "\n",
                "for i, (model_name, res) in enumerate(results.items()):\n",
                "    fpr, tpr, _ = roc_curve(y_test, res['y_pred_proba'])\n",
                "    ax.plot(fpr, tpr, \n",
                "            label=f\"{model_name} (AUC={res['auc_roc']:.3f})\",\n",
                "            linewidth=2, color=colors[i % len(colors)])\n",
                "\n",
                "# Add diagonal line\n",
                "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC=0.5)')\n",
                "\n",
                "# Add 70% target line\n",
                "ax.axhline(y=0.7, color='orange', linestyle=':', alpha=0.7, label='Target AUC=0.70')\n",
                "\n",
                "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
                "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
                "ax.set_title('ROC Curves Comparison - Credit Default Models', fontsize=14, fontweight='bold')\n",
                "ax.legend(loc='lower right', fontsize=10)\n",
                "ax.grid(True, alpha=0.3)\n",
                "ax.set_xlim([0, 1])\n",
                "ax.set_ylim([0, 1])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('Phase5_ROC_Curves.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n✓ ROC curves saved to Phase5_ROC_Curves.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.4 Best Model Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix for best model\n",
                "best_results = results[best_model_name]\n",
                "cm = confusion_matrix(y_test, best_results['y_pred'])\n",
                "\n",
                "print(f\"\\nConfusion Matrix - {best_model_name}:\")\n",
                "print(\"=\"*50)\n",
                "print(f\"True Negatives (TN):  {cm[0,0]:,}\")\n",
                "print(f\"False Positives (FP): {cm[0,1]:,}\")\n",
                "print(f\"False Negatives (FN): {cm[1,0]:,}\")\n",
                "print(f\"True Positives (TP):  {cm[1,1]:,}\")\n",
                "\n",
                "# Visualize confusion matrix\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
                "            xticklabels=['No Default', 'Default'],\n",
                "            yticklabels=['No Default', 'Default'],\n",
                "            annot_kws={'size': 16})\n",
                "ax.set_xlabel('Predicted', fontsize=12)\n",
                "ax.set_ylabel('Actual', fontsize=12)\n",
                "ax.set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('Phase5_Confusion_Matrix.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification Report\n",
                "print(f\"\\nClassification Report - {best_model_name}:\")\n",
                "print(\"=\"*60)\n",
                "print(classification_report(y_test, best_results['y_pred'], \n",
                "                          target_names=['No Default', 'Default']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.5 Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Importance\n",
                "model = best_results['model']\n",
                "\n",
                "if hasattr(model, 'feature_importances_'):\n",
                "    importances = model.feature_importances_\n",
                "    feat_imp_df = pd.DataFrame({\n",
                "        'Feature': features,\n",
                "        'Importance': importances\n",
                "    }).sort_values('Importance', ascending=True)\n",
                "    \n",
                "    # Plot\n",
                "    fig, ax = plt.subplots(figsize=(10, 8))\n",
                "    ax.barh(feat_imp_df['Feature'], feat_imp_df['Importance'], color='steelblue')\n",
                "    ax.set_xlabel('Importance', fontsize=12)\n",
                "    ax.set_title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
                "    ax.grid(True, alpha=0.3, axis='x')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('Phase5_Feature_Importance.png', dpi=150)\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\nTop 10 Features:\")\n",
                "    display(feat_imp_df.tail(10).sort_values('Importance', ascending=False))\n",
                "\n",
                "elif hasattr(model, 'coef_'):\n",
                "    coefs = model.coef_[0]\n",
                "    feat_imp_df = pd.DataFrame({\n",
                "        'Feature': features,\n",
                "        'Coefficient': coefs,\n",
                "        'Abs_Importance': np.abs(coefs)\n",
                "    }).sort_values('Abs_Importance', ascending=False)\n",
                "    \n",
                "    print(\"\\nTop 10 Features (by absolute coefficient):\")\n",
                "    display(feat_imp_df.head(10))\n",
                "else:\n",
                "    print(\"Feature importance not available for this model type.\")\n",
                "    feat_imp_df = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.6 Model Metrics Comparison Chart"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bar chart comparison\n",
                "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
                "x = np.arange(len(metrics))\n",
                "width = 0.2\n",
                "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "for i, (model_name, res) in enumerate(results.items()):\n",
                "    values = [res['accuracy'], res['precision'], res['recall'], res['f1_score'], res['auc_roc']]\n",
                "    ax.bar(x + i*width, values, width, label=model_name, alpha=0.8, color=colors[i])\n",
                "\n",
                "# Add target line for AUC\n",
                "ax.axhline(y=0.70, color='red', linestyle='--', alpha=0.7, label='AUC Target (0.70)')\n",
                "\n",
                "ax.set_xlabel('Metrics', fontsize=12)\n",
                "ax.set_ylabel('Score', fontsize=12)\n",
                "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
                "ax.set_xticks(x + width * 1.5)\n",
                "ax.set_xticklabels(metrics, fontsize=11)\n",
                "ax.legend(loc='upper left', fontsize=9)\n",
                "ax.set_ylim(0, 1.1)\n",
                "ax.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('Phase5_Metrics_Comparison.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save evaluation results for Phase 6\n",
                "evaluation_data = {\n",
                "    'comparison_df': comparison_df,\n",
                "    'best_model_name': best_model_name,\n",
                "    'best_auc': comparison_df['AUC-ROC'].max(),\n",
                "    'confusion_matrix': cm,\n",
                "    'feature_importance_df': feat_imp_df if 'feat_imp_df' in dir() else None,\n",
                "    'results': results,\n",
                "    'y_test': y_test,\n",
                "    'features': features\n",
                "}\n",
                "\n",
                "with open('phase5_evaluation.pkl', 'wb') as f:\n",
                "    pickle.dump(evaluation_data, f)\n",
                "\n",
                "print(\"\\n✓ Evaluation results saved to phase5_evaluation.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ✅ Phase 5 Complete\n",
                "\n",
                "**Evaluation Summary**:\n",
                "- All models evaluated and compared\n",
                "- ROC curves, confusion matrix, and feature importance generated\n",
                "- Best model identified with detailed analysis\n",
                "\n",
                "**Visualizations Created**:\n",
                "- Phase5_ROC_Curves.png\n",
                "- Phase5_Confusion_Matrix.png\n",
                "- Phase5_Feature_Importance.png\n",
                "- Phase5_Metrics_Comparison.png\n",
                "\n",
                "**Next**: Phase 6 - Deployment"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}